{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0f59cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "898b69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"Gender_Classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b2ded6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_path):\n",
    "  img = image.load_img(img_path,target_size=(180,180))\n",
    "  img_array = image.img_to_array(img)\n",
    "  img_batch = np.expand_dims(img_array, axis=0)\n",
    "  result = model.predict(img_batch)\n",
    "  if result >= 0.5:\n",
    "    return \"male\"\n",
    "  else:\n",
    "    return \"female\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0a8dda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 127ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"male.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef336f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 108ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"female.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76bfe770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step\n",
      "Gender: male\n"
     ]
    }
   ],
   "source": [
    "# Load the input image\n",
    "input_image = cv2.imread(\"male1.jpg\")\n",
    "\n",
    "# Save the input image to a file\n",
    "cv2.imwrite(\"input_image.jpg\", input_image)\n",
    "\n",
    "# Predict the gender of the person in the input image\n",
    "output = predict_image(\"input_image.jpg\")\n",
    "print(\"Gender:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d794ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the input image with the predicted gender class label\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "color = (0, 255, 0)\n",
    "thickness = 2\n",
    "text = \"Gender: {}\".format(output)\n",
    "cv2.putText(input_image, text, (25, 25), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "cv2.imshow(\"Input Image\", input_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a104c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_labels = ['female', 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aaa9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, (180,180))\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "    input_frame = np.expand_dims(normalized_frame, axis = 0)\n",
    "    \n",
    "    prediction = model.predict(input_frame)\n",
    "    predicted_gender = gender_labels[np.argmax(prediction)]\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_Scale = 1\n",
    "    color = (0,255,0)\n",
    "    thickness = 2\n",
    "    text = \"Gender : {}\".format(predicted_gender)\n",
    "    cv2.putText(frame,text, (50,50), font,font_Scale, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('Gender Classification', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcfa1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
